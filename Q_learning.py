import numpy as np
import random

# å®šç¾©è¿·å®®ï¼Œ'S' = èµ·é»ï¼Œ'G' = çµ‚é»ï¼Œ0 = é€šé“ï¼Œ1 = ç‰†å£
maze = np.array([
    ['S', 0, 0, 1, 0, 0, 0, 0, 0, 0],
    [1 , 1, 0, 1, 0, 1, 1, 1, 0, 0],
    [0 , 0, 0, 0, 1, 0, 0, 0, 0, 0],
    [0 , 1, 1, 0, 1, 1, 1, 0, 0, 0],
    [0 , 0, 0, 0, 0, 0, 0, 0, 0, 1],
    [1 , 1, 1, 1, 1, 1, 1, 1, 0, 0],
    [0 , 0, 0, 0, 0, 0, 0, 0, 0, 0],
    [0 , 1, 1, 1, 1, 1, 1, 1, 0, 1],
    [0 , 0, 0, 1, 0, 0, 0, 0, 0, 0],
    [1 , 1, 0, 0, 1, 1, 1, 1, 1, 'G']
])

# ä½ç½®è½‰æ›å‡½æ•¸
def pos_to_state(r, c): return r * 10 + c
def state_to_pos(s): return (s // 10, s % 10)

# æ‰¾å‡ºèµ·é»èˆ‡çµ‚é»ï¼Œä¸¦è½‰æˆæ•´æ•¸
start, goal = None, None
for r in range(10):
    for c in range(10):
        if maze[r, c] == 'S':
            start = pos_to_state(r, c)
            maze[r, c] = 0
        elif maze[r, c] == 'G':
            goal = pos_to_state(r, c)
            maze[r, c] = 0
maze = maze.astype(int)

# åˆå§‹åŒ– Reward çŸ©é™£
R = np.full((100, 100), -np.inf)
directions = [(-1,0), (1,0), (0,-1), (0,1)]

for r in range(10):
    for c in range(10):
        if maze[r, c] == 0:
            from_state = pos_to_state(r, c)
            for dr, dc in directions:
                nr, nc = r + dr, c + dc
                if 0 <= nr < 10 and 0 <= nc < 10 and maze[nr, nc] == 0:
                    to_state = pos_to_state(nr, nc)
                    R[from_state, to_state] = 100 if to_state == goal else -1

# åˆå§‹åŒ– Q-table
Q = np.zeros((100, 100))

# Q-learning åƒæ•¸
gamma = 0.8
alpha = 0.9
epsilon = 0.2
episodes = 1000

# è¨“ç·´
for _ in range(episodes):
    state = random.choice([s for s in range(100) if not np.all(np.isinf(R[s]))])
    while state != goal:
        actions = [a for a in range(100) if not np.isinf(R[state, a])]
        if not actions:
            break
        if random.random() < epsilon:
            next_state = random.choice(actions)
        else:
            next_state = max(actions, key=lambda a: Q[state, a])
        future_actions = [a for a in range(100) if not np.isinf(R[next_state, a])]
        max_q = max([Q[next_state, a] for a in future_actions], default=0)
        Q[state, next_state] += alpha * (R[state, next_state] + gamma * max_q - Q[state, next_state])
        state = next_state

# æ‰¾åˆ°æœ€çŸ­è·¯å¾‘
def extract_path(start, goal):
    path = [start]
    current = start
    visited = set()
    
    while current != goal:
        visited.add(current)
        next_states = [s for s in range(100) if not np.isinf(R[current, s])]
        if not next_states:
            break
        next_state = max(next_states, key=lambda s: Q[current, s])
        
        if next_state in visited or np.isinf(R[current, next_state]):
            break
        
        path.append(next_state)
        current = next_state
        
        if len(path) > 100:
            break
    return path

path = extract_path(start, goal)

# è¼¸å‡ºçµæœ
print("\nâœ… æœ€çŸ­è·¯å¾‘ (state index):", path)
print("ğŸ“ åº§æ¨™è·¯å¾‘:", [state_to_pos(s) for s in path])

# é¡¯ç¤ºè¿·å®®è·¯å¾‘ï¼ˆå¯è¦–åŒ–ï¼‰
display_maze = maze.astype(str)
display_maze[display_maze == '1'] = 'â–ˆ'
display_maze[display_maze == '0'] = ' '

# æ¨™è¨˜è·¯å¾‘
for s in path:
    r, c = state_to_pos(s)
    if (r, c) != state_to_pos(start) and (r, c) != state_to_pos(goal):  # ç¢ºä¿èµ·é»èˆ‡çµ‚é»ä¸è¢«è¦†è“‹
        display_maze[r, c] = '*'
display_maze[state_to_pos(start)] = 'S'
display_maze[state_to_pos(goal)] = 'G'

print("\nğŸ§­ è¿·å®®æœ€çŸ­è·¯å¾‘ï¼ˆæ–‡å­—ç‰ˆï¼‰ï¼š")
for row in display_maze:
    print(' '.join(row))
